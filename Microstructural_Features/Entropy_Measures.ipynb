{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b99887e9",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "> **Underlying Literature**: The following module was inspired by the ideas put forward in Chapter 18 of [Advances in Financial Machine Learning](https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086) by Marcos Lopez de Prado\n",
    "\n",
    "Entropy measures in financial sciences aim to discern how much information is contained in a given time series. That said, when markets are not perfect, prices are formed with partial information. As a result, entropy measures are helpful in determining just how much useful information is contained in said price signals.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- [Shannon Entropy](#shannon)\n",
    "- [Plug-in Entropy](#plug)\n",
    "- [Lempel-Ziv Entropy](#lz)\n",
    "- [Kontoyiannis Entropy](#konto)\n",
    "- [Binary Encoding](#binary)\n",
    "- [Quantile Encoding](#quantile)\n",
    "- [Sigma Encoding](#sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "079e043d",
   "metadata": {},
   "source": [
    "Before starting, we must first import our tick data from the sample data folder and generate tick classifications from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6df998-0330-458d-a4ba-a6ae93aa1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "\n",
    "from mlfinlab.microstructural_features import (first_generation as first_gen,\n",
    "                                               entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270ace9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_time\n",
       "2015-01-01 23:16:58.834    1.0\n",
       "2015-01-02 00:36:33.094   -1.0\n",
       "2015-01-02 01:54:08.770    1.0\n",
       "2015-01-02 04:28:09.015    1.0\n",
       "2015-01-02 06:57:53.850    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the tick data and only storing the closing price\n",
    "url = \"https://raw.githubusercontent.com/hudson-and-thames/example-data/main/tick_bars.csv\"\n",
    "tick_prices = pd.read_csv(url, index_col=0)['close']\n",
    "\n",
    "# Generating trade classifications for our tick data using the tick rule\n",
    "tick_classifications = first_gen.tick_rule(prices = tick_prices)\n",
    "\n",
    "# Previewing the tick classifications\n",
    "tick_classifications.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ef3f4dc",
   "metadata": {},
   "source": [
    "Now, we can apply our entropy measures to these tick classifications. For the purposes of this notebook, we'll be applying the functions to a subset of our tick classifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bb2065",
   "metadata": {},
   "source": [
    "## Shannon Entropy <a class=\"anchor\" id=\"shannon\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7a708f3",
   "metadata": {},
   "source": [
    "Claude Shannon is credited with having one of the first conceptualizations of entropy in 1948, which he defined as the\n",
    "average amount of information produced by a stationary source of data. More robustly defined, entropy is the smallest\n",
    "number of bits per character required to describe a message in a uniquely decodable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3aa9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895875212220555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Shannon entropy of a subset of the tick classifications\n",
    "shannon_entropy = entropy.get_shannon_entropy(message = tick_classifications[:100])\n",
    "\n",
    "# Previewing the Shannon entropy\n",
    "shannon_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db5902",
   "metadata": {},
   "source": [
    "## Plug-in Entropy <a class=\"anchor\" id=\"plug\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81929a04",
   "metadata": {},
   "source": [
    "Gao et al. (2008) built on the work done by Shannon by conceptualizing the Plug-in measure of entropy, also known as\n",
    "the maximum likelihood estimator of entropy. Given a data sequence $x_{1}^{n}$, comprising the string of values starting in position 1 and ending in position\n",
    ":$n$, we can form a dictionary of all words of length $w < n$ in that sequence: $A^w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6715172f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875257101057102"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Plug-in entropy of a subset of the tick classifications\n",
    "plug_in_entropy = entropy.get_plug_in_entropy(message = tick_classifications[:100])\n",
    "\n",
    "# Previewing the Plug-in entropy\n",
    "plug_in_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b158ff4",
   "metadata": {},
   "source": [
    "## Lempel-Ziv Entropy <a class=\"anchor\" id=\"lz\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd2e6615",
   "metadata": {},
   "source": [
    "Similar to Shannon entropy, Abraham Lempel and Jacob Ziv proposed in 1978 that entropy be treated as a measure of\n",
    "complexity. Intuitively, a complex sequence contains more information than a regular (predictable) sequence. Based on\n",
    "this idea, the Lempel-Ziv (LZ) algorithm decomposes a message into a number of non-redundant substrings. LZ entropy builds on this idea by dividing the number of non-redundant substrings by the length of the\n",
    "original message. The intuition here is that complex messages have high entropy, which will require large dictionaries\n",
    "of substrings relative to the length of the original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad10c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Lempel-Ziv entropy of a subset of the tick classifications\n",
    "lempel_ziv_entropy = entropy.get_lempel_ziv_entropy(message = tick_classifications[:100])\n",
    "\n",
    "# Previewing the Lempel-Ziv entropy\n",
    "lempel_ziv_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4059ce",
   "metadata": {},
   "source": [
    "## Kontoyiannis Entropy <a class=\"anchor\" id=\"konto\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ccc927a",
   "metadata": {},
   "source": [
    "In 1998 Kontoyiannis attempted to make more efficient use of the information available in a message by taking advantage of\n",
    "a technique known as length matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c099bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681763656863827"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Kontoyiannis entropy of a subset of the tick classifications\n",
    "kontoyiannis_entropy = entropy.get_konto_entropy(message = tick_classifications[:100])\n",
    "\n",
    "# Previewing the Konto entropy\n",
    "kontoyiannis_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad37fa34",
   "metadata": {},
   "source": [
    "Users may be wondering why some of the entropy functions are yielding values greater than 1. A comprehensive explanation of why this is occurring can be found in the StackExchange thread named [Why am I getting information entropy greater than 1?](https://stats.stackexchange.com/questions/95261/why-am-i-getting-information-entropy-greater-than-1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "263de2bd",
   "metadata": {},
   "source": [
    "The short explanation is that we are calculating our entropy measures using a log function with a base of 2, which has a maximum value that is greater than 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae701b32",
   "metadata": {},
   "source": [
    "Encoded messages can also be used to caluclate entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c722d4d",
   "metadata": {},
   "source": [
    "Before we start we need to import the packages used for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2575dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab895c71",
   "metadata": {},
   "source": [
    "## Binary Encoding<a class=\"anchor\" id=\"binary\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7e18fd",
   "metadata": {},
   "source": [
    "In the case of returns series derived from price bars (i.e., bars containing prices fluctuating between two symmetric\n",
    "horizontal barriers, centered around the start price), binary encoding occurs naturally because the value of $|r_{t}|$\n",
    "is roughly constant. For example, a stream of returns :math:`r_{t}` can be encoded according to the sign,\n",
    "with 1 indicating $r_{t} > 0$ and $0$ indicating $r_{t} < 0$, thus eliminating occurrences where $r_{t} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cbd35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug-in Entropy 0.9875257101057102\n"
     ]
    }
   ],
   "source": [
    "# Import MlFinLab tools\n",
    "from mlfinlab.microstructural_features.encoding import encode_tick_rule_array\n",
    "\n",
    "# Create data and use tools\n",
    "values = tick_classifications[:100].values\n",
    "encoded_tick_rule = encode_tick_rule_array(values)\n",
    "\n",
    "# Plug-in Entropy \n",
    "plug_in_entropy_binary_message = entropy.get_plug_in_entropy(message = encoded_tick_rule)\n",
    "print('Plug-in Entropy', plug_in_entropy_binary_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e657976",
   "metadata": {},
   "source": [
    "## Quantile Encoding<a class=\"anchor\" id=\"quantile\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e798e75c",
   "metadata": {},
   "source": [
    "Unless price bars are employed, more than two codes are likely to be required. One method is to assign a code to each\n",
    "$r_{t}$ based on the quantile to which it belongs. An in-sample period is used to calculate the quantile limits (training set).\n",
    "In the overall in-sample, each letter will receive the same amount of observations, and out-of-sample, each letter will\n",
    "receive close to the same number of observations. Some codes span a larger portion of $r_{t}$'s range than others when utilizing\n",
    "the approach. This uniform (in-sample) or nearly uniform (out-of-sample) distribution of codes increases average entropy readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481634d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug-in Entropy 0.9875257101057102\n"
     ]
    }
   ],
   "source": [
    "# Import MlFinLab tools\n",
    "from mlfinlab.microstructural_features.encoding import (quantile_mapping, encode_array)\n",
    "\n",
    "# Create data and use tools\n",
    "values = tick_classifications[:100].values\n",
    "quantile_dict = quantile_mapping(values, num_letters=10)\n",
    "message = encode_array(values, quantile_dict)\n",
    "\n",
    "# Plug-in Entropy \n",
    "plug_in_entropy_quantile_message = entropy.get_plug_in_entropy(message = message)\n",
    "print('Plug-in Entropy', plug_in_entropy_quantile_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e800064e",
   "metadata": {},
   "source": [
    "## Sigma Encoding<a class=\"anchor\" id=\"sigma\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb715cf7",
   "metadata": {},
   "source": [
    "Rather than limiting the amount of codes, we may instead let the price stream define the actual vocabulary.\n",
    "Let's say we want to fix a discretization step, ${\\sigma}$.\n",
    "Then we assign 0 to $r_{t} \\in[\\min \\{r\\}, \\min \\{r\\}+\\sigma), 1$ to $r_{t} \\in$\n",
    "$[\\min \\{r\\}+\\sigma, \\min \\{r\\}+2 \\sigma$,\n",
    "and so on until every observation has been encoded with a total of ceil\n",
    "$\\left[\\frac{\\max \\{r\\}-\\min \\{r\\}}{\\sigma}\\right]$.\n",
    "The ceil[.] denotes the ceiling function.\n",
    "Unlike quantile encoding, each code now covers the same fraction of the range of $r_{t}$'s.\n",
    "Entropy readings will be smaller than in quantile encoding on average because codes are not uniformly distributed;\n",
    "however, the introduction of a \"rare\" code will cause entropy measurements to surge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d9d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug-in Entropy 0.9875257101057102\n"
     ]
    }
   ],
   "source": [
    "# Import MlFinLab tools\n",
    "from mlfinlab.microstructural_features.encoding import (sigma_mapping, encode_array)\n",
    "\n",
    "# Create data and use tools\n",
    "values = tick_classifications[:100].values\n",
    "sigma_dict = sigma_mapping(values, step=1) \n",
    "message = encode_array(values, sigma_dict)\n",
    "\n",
    "# Plug-in Entropy \n",
    "plug_in_entropy_sigma_message = entropy.get_plug_in_entropy(message = message)\n",
    "print('Plug-in Entropy', plug_in_entropy_sigma_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "febec076",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook describes different entropy measures and message encoding methods implemented in the MlFinLab package.\n",
    "\n",
    "These estimators have been originally presented in the book \"Advances in Financial Machine Learning\" by Marcos Lopez De Prado (https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086).\n",
    "\n",
    "Key takeaways from the notebook:\n",
    "\n",
    "* Entropy measures are helpful in determining just how much useful information is contained in said price signals.\n",
    "\n",
    "* Entropy output an be greater than 1 in some cases. \n",
    "\n",
    "* Binary, Quantile and Sigma ecoded messages can be used to claculate these Entropy measures.\n",
    "\n",
    "* Quantile and Sigma encoding need an additional function to encode a dictionary before it can be used to claculate the Entropy measures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35fedb99-ebb3-4d04-8dee-18daa6b1889b",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* Lopez de Prado, M. (2018) Advances in Financial Machine Learning. New York, NY: John Wiley & Sons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
